{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "881f40b3-7c50-47d8-b23f-74e36d63f5c0",
   "metadata": {},
   "source": [
    "Importing Neccessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41a2edeb-7b09-4c4b-b36e-0b79b72e1e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cececc9-d46d-4420-9864-ebb54eaef6a8",
   "metadata": {},
   "source": [
    "Loading ENV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5188c6e-8ddb-4022-85d1-4264971b2d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override = True)\n",
    "\n",
    "api_key = os.getenv('LLAMA')\n",
    "\n",
    "MODEL = 'meta-llama/llama-4-maverick:free'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a672b680-5072-464b-bd3c-03d22bf9ff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key= api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ff5a6be-cfa2-4455-9d89-b76bebff3645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_interview_prep(job_title, company, job_description):\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert career coach and interview simulator, acting as a hiring manager.\n",
    "    Your task is to generate highly relevant, role-specific interview questions based on the provided job description.\n",
    "    For each question, provide a detailed model answer structured using the STAR method (Situation, Task, Action, Result) where applicable.\n",
    "    Ensure the questions cover both technical and behavioral aspects mentioned in the description.\n",
    "    Format your output clearly using Markdown.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "    Please generate interview preparation material for the following role.\n",
    "\n",
    "    **Job Title:** {job_title}\n",
    "    **Company:** {company}\n",
    "\n",
    "    **Job Description:**\n",
    "    ---\n",
    "    {job_description}\n",
    "    ---\n",
    "\n",
    "    Generate 5 relevant questions (a mix of technical and behavioral) with detailed model answers based *only* on the skills and responsibilities in this job description.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "          ],\n",
    "    )\n",
    "    result = response.choices[0].message.content\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "519bfa68-2d8a-4d01-8b0f-3f6ceedc4ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Interview Preparation Material for Machine Learning Engineer at NeuraTech Labs\n",
       "\n",
       "#### Question 1: Technical - Designing a Recommendation System\n",
       "**Can you describe your experience with designing and implementing a recommendation system? How did you handle cold start problems and ensure scalability?**\n",
       "\n",
       "*Model Answer (STAR Method):*\n",
       "- **Situation**: In my previous role at XYZ Corp, I was tasked with improving user engagement on our e-commerce platform through a recommendation system.\n",
       "- **Task**: The goal was to design a system that could recommend products to users based on their past purchases and browsing history, handling both new users and new products (cold start problem).\n",
       "- **Action**: I implemented a hybrid recommendation system combining collaborative filtering and content-based filtering. For the cold start problem, I used a knowledge graph-based approach to leverage product attributes and user demographics. To ensure scalability, the system was built using TensorFlow and deployed on AWS SageMaker, utilizing distributed training and optimized inference.\n",
       "- **Result**: The system improved click-through rates by 25% and increased average order value by 15%. It handled millions of users and products efficiently, demonstrating its scalability.\n",
       "\n",
       "#### Question 2: Behavioral - Collaboration and Communication\n",
       "**Describe a time when you had to collaborate with cross-functional teams (e.g., data scientists, software engineers, product managers) to integrate an ML model into a production system. What challenges did you face, and how did you address them?**\n",
       "\n",
       "*Model Answer (STAR Method):*\n",
       "- **Situation**: At my previous company, ABC Inc., I worked on integrating a predictive maintenance model into our IoT platform.\n",
       "- **Task**: The task involved collaboration with data scientists who developed the model, software engineers responsible for the platform's backend, and product managers who defined the requirements.\n",
       "- **Action**: I led the effort to containerize the model using Docker and worked closely with the backend team to develop a FastAPI for model serving. Regular meetings were held to ensure all teams were aligned with the project's progress and challenges. I also facilitated knowledge sharing sessions to educate the team on ML concepts and deployment strategies.\n",
       "- **Result**: The model was successfully deployed, reducing equipment downtime by 30%. The collaboration ensured a smooth integration process, meeting the project's deadline and exceeding the expected accuracy thresholds.\n",
       "\n",
       "#### Question 3: Technical - Model Evaluation and Improvement\n",
       "**How do you evaluate the performance of a machine learning model, and what strategies do you employ to improve its accuracy and latency?**\n",
       "\n",
       "*Model Answer (STAR Method):*\n",
       "- **Situation**: While working on a personalization project, I developed a model to predict user preferences.\n",
       "- **Task**: The task was to evaluate the model's performance and identify areas for improvement.\n",
       "- **Action**: I used metrics such as precision, recall, F1-score, and A/B testing to evaluate the model's performance. To improve accuracy, I experimented with different deep learning architectures using PyTorch and fine-tuned hyperparameters. For latency improvement, I optimized the model using quantization and pruning techniques and deployed it on a cloud platform (GCP) with optimized serving configurations.\n",
       "- **Result**: The model's F1-score improved by 10%, and the inference latency was reduced by 40%, enhancing the overall user experience.\n",
       "\n",
       "#### Question 4: Technical - MLOps and Model Deployment\n",
       "**Can you discuss your experience with MLOps tools and practices? How have you used tools like MLflow, Docker, or Kubernetes in your ML projects?**\n",
       "\n",
       "*Model Answer (STAR Method):*\n",
       "- **Situation**: In a project at DEF Startups, we were developing an ML-powered chatbot.\n",
       "- **Task**: The task was to manage the ML lifecycle efficiently, from experimentation to deployment.\n",
       "- **Action**: I used MLflow to track experiments, manage model versions, and reproduce results. Docker was utilized for containerizing the model and ensuring consistency across different environments. Kubernetes was employed for orchestrating the deployment, ensuring scalability and reliability.\n",
       "- **Result**: The use of MLOps tools streamlined our development process, reduced deployment time by 50%, and improved model version management and reproducibility.\n",
       "\n",
       "#### Question 5: Technical - Handling Large-Scale Datasets\n",
       "**Explain how you would handle a large-scale dataset for training a machine learning model, including data preprocessing and feature engineering steps.**\n",
       "\n",
       "*Model Answer (STAR Method):*\n",
       "- **Situation**: At GHI Analytics, I worked on a project involving a large dataset of user interactions for building a recommendation system.\n",
       "- **Task**: The task was to preprocess and engineer features from the dataset for model training.\n",
       "- **Action**: I started by cleaning the data, handling missing values, and removing outliers. Feature engineering involved creating new features from existing ones (e.g., user engagement metrics) and transforming categorical variables using techniques like one-hot encoding. I used NumPy and Pandas for efficient data manipulation and SQL for querying the dataset. Data preprocessing and feature engineering pipelines were developed using scikit-learn.\n",
       "- **Result**: The preprocessed and engineered features significantly improved the model's performance, achieving a higher accuracy and better generalization capability. The processes were also documented and made reproducible for future projects."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_interview_prep(\"Machine Learning Engineer\",\"NeuraTech Labs\", jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67c01cb0-4e33-406c-abef-4012878030aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MANISH JAISWAL\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\interface.py:414: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated. Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# app.py (continued)\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "# --- Gradio UI ---\n",
    "\n",
    "# Define the input components\n",
    "job_title_input = gr.Textbox(label=\"Job Title\", placeholder=\"e.g., Senior Software Engineer\")\n",
    "company_input = gr.Textbox(label=\"Company\", placeholder=\"e.g., Google\")\n",
    "job_description_input = gr.Textbox(\n",
    "    label=\"Paste Full Job Description Here\", \n",
    "    lines=15, \n",
    "    placeholder=\"Paste the job description from LinkedIn, Indeed, etc.\"\n",
    ")\n",
    "\n",
    "# Define the output component\n",
    "output_display = gr.Markdown(label=\"Your Interview Prep Guide\")\n",
    "\n",
    "\n",
    "\n",
    "# (Your existing code for imports, the generate_interview_prep function, \n",
    "# and the Gradio component definitions goes here)\n",
    "\n",
    "# --- Gradio UI (for Jupyter Notebook) ---\n",
    "\n",
    "# Create the Gradio Interface (this part is the same)\n",
    "iface = gr.Interface(\n",
    "    fn=generate_interview_prep,\n",
    "    inputs=[job_title_input, company_input, job_description_input],\n",
    "    outputs=output_display,\n",
    "    title=\"AI Interview Prep Chatbot\",\n",
    "    description=\"Enter a job title, company, and the full job description to get tailored interview questions and model answers.\",\n",
    "    allow_flagging=\"never\",\n",
    "    theme=gr.themes.Soft()\n",
    ")\n",
    "\n",
    "# Launch the app INLINE for Jupyter Notebook\n",
    "# This will display the interactive UI directly in the cell's output.\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10076b39-2e85-4d4c-b581-0e3a9e15a704",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
